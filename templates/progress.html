{% extends "layout.html" %}
{% block content %}

	<div align='left'>
		<h1>Initial analysis</h1>
		<div>
			<p>
				My preliminary analysis focused on two datasets: a 714MB dataset from Instacart of their grocery orders and a 15MB recipe dataset from Yummly.
			</p>
			<p>
				Plots can be seen <a href='https://jdh-prelim.herokuapp.com/graphs/frequency_comparison_graph.html'> <b>here</b></a> and <a href='https://jdh-prelim.herokuapp.com/graphs/repeated_frac_graph.html'><b>here</b></a>
			</p>
		</div>

		<h1>Since we last spoke</h1>
		<div>
			<h3>Web scrapping</h3>
			<p>
				My recipe data set was limited in size (only 15MB and xxx recipes), so I put all of my efforts into building a web scraper. My primary target was <a href='allrecipes.com'><b>allrecipes.com</b></a> as they are massive, and well known, resource for recipes. Unfortunately, traditional web scrapping tools like <a href='https://scrapy.org/'><b>scrapy</b></a> didn't work due to the construction of their site.
			</p>
			<p>
				The core problem, at least as far as I was able to figure, is that tools like scrapy work best when there is a clearly defined structure that you can predict and then crawl. For instance, when I google something, at the bottom of results page I know that there will be a next button that I can hit to get the next page. With allrecipes.com, the instead follow a pinterist infinite scroll type layout. In order to get more data, you need to interact with the site.
			</p>
			<h3>A hidden API</h3>
			<p>
				To get around this problem, I used the developer tools in Chrome to look at the network traffic to/from the site when I scrolled to the bottom of a list of recipes. After a bit of analysis I was able to find what my browser was sending and worked out the precise data that I have to send to effectively get a <i>page of results</i>.
			</p>
			<p>
				With this knowledge in hand, I built an automatic tool to build up a collection of recipe URLs and stored them in a Mongo database. It turns out the 'recipe cards' that are returned are not always the same type or format so I've had to do some data cleaning, but I have already collected over 10,000 recipe urls. With the urls I can then use tools beautifulsoup to parse out the ingredients and other relevant data. The final step of parsing out the ingredients is currently only partially done.
			</p>
		</div>
	</div>

{% endblock %}